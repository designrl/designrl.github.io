<!doctype html>
<meta charset="utf-8">
<style>
body {
  overflow-x: hidden;
}
.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}
.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}
.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}
.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}
</style>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <!--<link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>-->

  <meta name="theme-color" content="#ffffff" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-127184810-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-127184810-1');
  </script>
  <!-- SEO -->
  <meta property="og:title" content="RL for Improving Agent Design" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="What happens when we let an agent learn a better body design?" />
  <meta property="og:image" content="https://designrl.github.io/assets/img/biped_card_rect_v3.png" />
  <meta property="og:url" content="https://designrl.github.io/" />
  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="RL for Improving Agent Design" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="RL for Improving Agent Design" />
  <meta name="twitter:image" content="https://designrl.github.io/assets/img/biped_card_square_v2.png" />

</head>
<link rel="stylesheet" href="css/katex.min.css">

<!--<script src="lib/jquery-1.12.4.min.js"></script>
<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>

<script type="text/front-matter">
  title: "RL for Improving Agent Design"
  description: ""
</script>
<body>
  <div id="no_javasript_warning">
    <h3>This page requires Javascript. Please enable it for <code>https://designrl.github.io/</code></h3>
  </div>
  <script>
    document.getElementById("no_javasript_warning").style.display = "none";
  </script>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/bipedhard_compare_vs_augment.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>

<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: center;">Original agent in BipedalWalkerHardcore-v2 learning to navigate.</figcaption></td>
<td width="50%"><figcaption style="text-align: center;">Agent learns a better body design jointly with the navigation task.</figcaption></td>
</tr></table>

</div>

<dt-article id="dtbody">

<dt-byline class="l-page transparent"></dt-byline>
<h1>Reinforcement Learning<br/>for Improving Agent Design</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="http://blog.otoro.net/">David Ha</a>
        <a class="affiliation" href="https://g.co/brain">Google Brain</a>
    </div>
  </div>
  <div class="date">
    <div class="month">Oct 10</div>
    <div class="year">2018</div>
  </div>
  <div class="date">
    <div class="month">Download</div>
    <div class="year" style="color: #FF6C00;"><a href="https://arxiv.org/abs/1810.03779" target="_blank">PDF</a></div>
  </div>
  <div class="date">
    <div class="month">Artificial Life</div>
    <div class="year" style="color: #FF6C00;"><a href="https://doi.org/10.1162/artl_a_00301" target="_blank">MIT Press</a></div>
  </div>
  <div class="date">
    <div class="month" style="color: #668;"><a href="#citation">BibTeX</a></div>
    <div class="year" style="color: #668;"><a href="#citation">Citation</a></div>
  </div>
</div>
</dt-byline>
</dt-byline><h2>Abstract</h2>
<p>In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose a minor modification to the Gym<dt-cite key="openai_gym"></dt-cite> framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications.</p>
<hr>
<h2>Introduction</h2>
<p>Embodied cognition<dt-cite key="anderson2003embodied,mahon2008critical,shapiro2010embodied"></dt-cite> is the theory that an organism's cognitive abilities is shaped by its body. It is even argued that an agent's cognition extends beyond its brain, and is strongly influenced by aspects of its body and also the experiences from its various sensorimotor functions<dt-cite key="wilson2002six,gover1996embodied"></dt-cite>. Evolution plays a vital role in shaping an organism's body to adapt to its environment; the brain and its ability to learn is only one of many body components that is co-evolved together<dt-cite key="pfeifer2006body"></dt-cite>. We can observe embodiment in nature by observing that many organisms exhibit complex motor skills, such as the ability to jump<dt-cite key="bresadola1998medicine"></dt-cite> or swim<dt-cite key="beal2006passive"></dt-cite> even after brain death.</p>
<p>While evolution shapes the overall structure of the body of a particular species, an organism can also change and adapt its body to its environment during its life. For instance, professional athletes spend their lives body training while also improving specific mental skills required to master a particular sport<dt-cite key="tricoli2005short"></dt-cite>. In everyday life, regular exercise not only strengthens the body but also improves mental conditions<dt-cite key="raglin1990exercise,deslandes2009exercise"></dt-cite>. We not only learn and improve our skills and abilities in our everyday lives, but also learn to shape our bodies for the lives we want to live.</p>
<div style="text-align: center;">
<table style="width:100%;" cellspacing="0" cellpadding="0"><tr>
<td width="33%"><figcaption style="text-align: left;">Original design<dt-cite key="bipedalwalker"></dt-cite>:</figcaption></td>
<td width="33%"><figcaption style="text-align: left;">Learned design:</figcaption></td>
<td width="34%"><figcaption style="text-align: left;">Optimized for smaller leg area:</figcaption></td>
</tr></table>
<img src="assets/img/biped_easy.png" style="display: block; margin: auto; width: 100%;"/>
<table style="width:100%;" cellspacing="0" cellpadding="0"><tr>
<td width="33%"><figcaption style="text-align: left;"><i>Hardcore</i><dt-cite key="bipedalwalkerhardcore"></dt-cite> version:</figcaption></td>
<td width="33%"><figcaption style="text-align: left;">Learned design:</figcaption></td>
<td width="34%"><figcaption style="text-align: left;">Optimized for smaller leg area:</figcaption></td>
</tr></table>
<img src="assets/img/biped_hard.png" style="display: block; margin: auto; width: 100%;"/>
</div>
<p>We are interested to investigate embodied cognition within the reinforcement learning (RL) framework. Most baseline tasks<dt-cite key="todorov2012mujoco,roboschool"></dt-cite> in the RL literature test an algorithm's ability to learn a policy to control the actions of an agent, with a predetermined body design, to accomplish a given task inside an environment. The design of the agent's body is rarely optimal for the task, and sometimes even intentionally designed to make policy search challenging. In this work, we explore enabling learning versions of an agent's body that are better suited for its task, jointly with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better for its task, but also facilitates policy learning. We can even optimize our agent's body for certain desired characteristics, such as material usage. Our approach may help uncover design principles useful for assisted-design.</p>
<p>Furthermore, we believe the ability to learn useful morphology is an important area for the advancement of AI. Although morphology learning originally initiated from the field of evolutionary computation, there has also been great advances in RL in recent years, and we believe much of what happens in ALife should be in principle be of interest to the RL community and vice versa, since learning and evolution are just two sides of the same coin.</p>
<p>We believe that conducting experiments using standardized simulation environments facilitate the communication of ideas across disciplines, and for this reason we design our experiments based on applying ideas from ALife, namely morphology learning, to standardized tasks in the OpenAI Gym environment, a popular testbed for conducting experiments in the RL community. We decide to use standardized Gym environments such as Ant (based on Bullet physics engine) and Bipedal Walker (based on Box2D) not only for their simplicity, but also because their difficulty is well-understood due to the large number of RL publications that use them as benchmarks. As we shall see later, the <em>BipedalWalkerHardcore-v2</em> task, while simple looking, is especially difficult to solve with modern Deep RL methods. By applying simple morphology learning concepts from ALife, we are able to make a difficult task solvable with much fewer compute resources. We also made the code for augmenting OpenAI Gym for morphology learning, along with all pretrained models for reproducing results in this paper available.</p>
<p>We hope this paper can serve as a catalyst to precipitate a cultural shift in both fields and encourage researchers to open up our minds to each other. By drawing ideas from ALife and demonstrating them in the OpenAI Gym platform used by RL, we hope this work can set an example to bring both the RL and ALife communities closer together to find synergies and push the AI field forward.</p>
<hr>
<h2>Related Work</h2>
<p>There is a broad literature in evolutionary computation, artificial life and robotics devoted to studying, and modelling embodied cognition<dt-cite key="pfeifer2006body"></dt-cite>. In 1994, Karl Sims demonstrated that artificial evolution can produce novel morphologies that resemble organisms observed in nature<dt-cite key="sims1994evolving,sims1994evolving_MIT"></dt-cite>. Later works further investigated morphology evolution<dt-cite key="bongard2011morphological,auerbach2012relationship,auerbach2014environmental,leger1999automated,szerlip2013indirectly,szerlip2014steps,moore2014evolutionary,boxcar2d,auerbach2014robogen"></dt-cite>, modular robotics<dt-cite key="lipson2000automatic,ostergaard2003evolving,prokopenko2006evolving,zykov2007evolved"></dt-cite>, and evolving soft robots<dt-cite key="cheney2013unshackling,corucci2018evolving"></dt-cite> using indirect encoding<dt-cite key="neat,hyperneat,auerbach2010evolving,auerbach2011evolving,auerbach2010dynamic"></dt-cite>.</p>
<div style="text-align: center;">
<!--<video autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;"><source src="assets/mp4/related_work.mp4" type="video/mp4"/></video>-->
<video class="b-lazy" data-src="assets/mp4/related_work.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Evolved Virtual Creatures<dt-cite key="sims1994evolving"></dt-cite>, Soft Robots<dt-cite key="cheney2013unshackling,corucci2018evolving"></dt-cite>, <i>Strandbeest</i><dt-cite key="jansen2008strandbeests"></dt-cite> and Passive Walkers<dt-cite key="mcgeer1990passive,collins2001three"></dt-cite>.</figcaption>
</div>
<p>Literature in the area of passive dynamics study robot designs that rely on natural swings of motion of body components instead of deploying and controlling motors at each joint <dt-cite key="mcgeer1990passive,collins2001three,paul2004morphology,collins2005efficient"></dt-cite>. Notably, artist Theo Jansen <dt-cite key="jansen2008strandbeests"></dt-cite> also employed evolutionary computation to design physical <em>Strandbeests</em> that can walk on their own consuming only wind energy to raise environmental awareness.</p>
<p>Recent works in robotics investigate simultaneously optimizing body design and control of a legged robot <dt-cite key="ha2017joint,ha2018computational"></dt-cite> using constraint-based modelling, which is related to our RL-based approach. Related to our work,  <dt-cite key="geijtenbeek2013flexible,agrawal2014diverse"></dt-cite> employ CMA-ES <dt-cite key="cmaes"></dt-cite> to optimize over both the motion control and physical configuration of agents. A related recent work <dt-cite key="schaff2018jointly,schaff2018jointly_iclr_workshop"></dt-cite> employs RL to learn both the policy and design parameters in an alternating fashion, where a single shared policy controls a distribution of different designs, while in this work we simply treat both policy and design parameters the same way.</p>
<hr>
<h2>Method</h2>
<p>In this section, we describe the method used for learning a version of the agent's design better suited for its task jointly with its policy. In addition to the weight parameters of our agent's policy network, we will also parameterize the agent's environment, which includes the specification of the agent's body structure. This extra parameter vector, which may govern the properties of items such as width, length, radius, mass, and orientation of an agent's body parts and their joints, will also be treated as a learnable parameter. Hence the weights <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> we need to learn will be the parameters of the agent's policy network combined with the environment's parameterization vector. During a rollout, an agent initialized with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> will be deployed in an environment that is also parameterized  with the same parameter vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span>.</p>
<p>The goal is to learn <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> to maximize the expected cumulative reward, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo>[</mo><mi>R</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>]</mo></mrow><annotation encoding="application/x-tex">E[R(w)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span>, of an agent acting on a policy with parameters <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> in an environment governed by the same <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span>. In our approach, we search for <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> using a population-based policy gradient method based on Section 6 of Williams' 1992 REINFORCE<dt-cite key="williams1992"></dt-cite>. The next section provides an overview of this algorithm.</p>
<div style="text-align: center;">
<dt-code block language="python">
def rollout(agent, envparams, env):
  env.augment(envparams)
  obs = env.reset()
  done = False
  cumulative_reward = 0
  while not done:
    a = agent.action(obs)
    obs, r, done = env.step(a)
    r = augment_reward(r, envparams)
    cumulative_reward += r
  return cumulative_reward
</dt-code>
<figcaption style="text-align: left;">Using a modified OpenAI Gym<dt-cite key="openai_gym"></dt-cite>framework, we parameterize parts of an environment, and allow an agent to modify its environment before a rollout, and also augment its reward based on these parameters.</figcaption>
</div>
<p>Armed with the ability to change the design configuration of an agent's own body, we also wish to explore encouraging the agent to challenge itself by rewarding it for trying more difficult designs. For instance, carrying the same payload using smaller legs may result in a higher reward than using larger legs. Hence the reward given to the agent may also be augmented according to its parameterized environment vector. We will discuss reward augmentation to optimize for desirable design properties later on in more detail later on.</p>
<hr>
<h2>Overview of Population-based Policy Gradient Method (REINFORCE)</h2>
<p>In this section we provide an overview of the population-based policy gradient method described in Section 6 of William's REINFORCE<dt-cite key="williams1992"></dt-cite> paper for learning a parameter vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> in a reinforcement learning environment. In this approach, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> is sampled from a probability distribution <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi><mo>(</mo><mi>w</mi><mo separator="true">,</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\pi(w, \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span> parameterized by <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span>. We define the expected cumulative reward <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.00773em;">R</span></span></span></span> as:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><msub><mi>E</mi><mrow><mi>w</mi></mrow></msub><mo>[</mo><mi>R</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>]</mo><mo>=</mo><mo>∫</mo><mi>R</mi><mo>(</mo><mi>w</mi><mo>)</mo><mspace width="0.277778em"></mspace><mi>π</mi><mo>(</mo><mi>w</mi><mo separator="true">,</mo><mi>θ</mi><mo>)</mo><mspace width="0.277778em"></mspace><mi>d</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">J(\theta) = E_{w}[R(w)] = \int R(w) \; \pi(w, \theta) \; dw</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.805em;"></span><span class="strut bottom" style="height:1.11112em;vertical-align:-0.30612em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.05764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose">]</span><span class="mrel">=</span><span class="op-symbol small-op mop" style="margin-right:0.19445em;top:-0.0005599999999999772em;">∫</span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mord mspace thickspace"></span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mord mspace thickspace"></span><span class="mord mathit">d</span><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span></p>
<p>Using the <em>log-likelihood trick</em> allows us to write the gradient of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span> with respect to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span>:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><msub><mi>E</mi><mrow><mi>w</mi></mrow></msub><mo>[</mo><mspace width="0.277778em"></mspace><mi>R</mi><mo>(</mo><mi>w</mi><mo>)</mo><mspace width="0.277778em"></mspace><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>log</mi><mi>π</mi><mo>(</mo><mi>w</mi><mo separator="true">,</mo><mi>θ</mi><mo>)</mo><mspace width="0.277778em"></mspace><mo>]</mo></mrow><annotation encoding="application/x-tex">\nabla_{\theta} J(\theta) = E_{w}[ \; R(w)  \; \nabla_{\theta} \log \pi(w, \theta) \; ]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.05764em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">[</span><span class="mord mspace thickspace"></span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mord mspace thickspace"></span><span class="mclose">]</span></span></span></span></p>
<p>In a population size of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span>, where we have solutions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">w^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">w^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, ..., <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>w</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">w^N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8413309999999999em;"></span><span class="strut bottom" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, we can estimate this as:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>≈</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><mspace width="0.277778em"></mspace><mi>R</mi><mo>(</mo><msup><mi>w</mi><mi>i</mi></msup><mo>)</mo><mspace width="0.277778em"></mspace><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>log</mi><mi>π</mi><mo>(</mo><msup><mi>w</mi><mi>i</mi></msup><mo separator="true">,</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; R(w^i)  \; \nabla_{\theta} \log \pi(w^i, \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">≈</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.394em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop"><span class="op-symbol small-op mop" style="top:-0.0000050000000000050004em;">∑</span><span class="vlist"><span style="top:0.30001em;margin-left:0em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-0.364em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mspace thickspace"></span><span class="mord mathit" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></p>
<p>With this approximated gradient <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\nabla_{\theta} J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>, we then can optimize <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span> using gradient ascent:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mo>→</mo><mi>θ</mi><mo>+</mo><mi>α</mi><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\theta \rightarrow \theta + \alpha \nabla_{\theta} J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mrel">→</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></p>
<p>and sample a new set of candidate solutions <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span> from updating the pdf using learning rate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.0037em;">α</span></span></span></span>. We follow the approach in REINFORCE where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">π</span></span></span></span> is modelled as a factored multi-variate normal distribution. Williams derived closed-form formulas of the gradient <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>log</mi><mi>π</mi><mo>(</mo><msup><mi>w</mi><mi>i</mi></msup><mo separator="true">,</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\nabla_{\theta} \log \pi(w^i, \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.824664em;"></span><span class="strut bottom" style="height:1.0746639999999998em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>. In this special case, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span> will be the set of mean <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">μ</span></span></span></span> and standard deviation <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">σ</span></span></span></span> parameters. Therefore, each element of a solution can be sampled from a univariate normal distribution <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>∼</mo><mi>N</mi><mo>(</mo><msub><mi>μ</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>σ</mi><mi>j</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">w_j \sim N(\mu_j, \sigma_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mrel">∼</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">μ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>. Williams derived the closed-form formulas for the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><mi>θ</mi></mrow></msub><mi>log</mi><mi>N</mi><mo>(</mo><msup><mi>z</mi><mi>i</mi></msup><mo separator="true">,</mo><mi>θ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\nabla_{\theta} \log N(z^i, \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.824664em;"></span><span class="strut bottom" style="height:1.0746639999999998em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span> term for each individual <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">μ</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.03588em;">σ</span></span></span></span> element of vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span> on each solution <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span> in the population:</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>μ</mi><mrow><mi>j</mi></mrow></msub></mrow></msub><mi>log</mi><mi>N</mi><mo>(</mo><msup><mi>w</mi><mi>i</mi></msup><mo separator="true">,</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msubsup><mi>w</mi><mi>j</mi><mi>i</mi></msubsup><mo>−</mo><msub><mi>μ</mi><mi>j</mi></msub></mrow><mrow><msubsup><mi>σ</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\nabla_{\mu_{j}} \log N(w^i, \theta) = \frac{w_j^i - \mu_j}{\sigma_j^2},</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.17874em;"></span><span class="strut bottom" style="height:1.84994em;vertical-align:-0.6712em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.14999999999999997em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit">μ</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.37358em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.2862857142857143em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.34479999999999994em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.59488em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.26808571428571426em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.363em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">μ</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mpunct">,</span></span></span></span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>σ</mi><mrow><mi>j</mi></mrow></msub></mrow></msub><mi>log</mi><mi>N</mi><mo>(</mo><msup><mi>w</mi><mi>i</mi></msup><mo separator="true">,</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mo>(</mo><msubsup><mi>w</mi><mi>j</mi><mi>i</mi></msubsup><mo>−</mo><msub><mi>μ</mi><mi>j</mi></msub><msup><mo>)</mo><mn>2</mn></msup><mo>−</mo><msubsup><mi>σ</mi><mi>j</mi><mn>2</mn></msubsup></mrow><mrow><msubsup><mi>σ</mi><mi>j</mi><mn>3</mn></msubsup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\nabla_{\sigma_{j}} \log N(w^i, \theta) = \frac{(w_j^i - \mu_j)^2 - \sigma_j^2}{\sigma_j^3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.17874em;"></span><span class="strut bottom" style="height:1.84994em;vertical-align:-0.6712em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:0.14999999999999997em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.37358em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.2862857142857143em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.34479999999999994em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathrm">3</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.59488em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.26808571428571426em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.363em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">μ</span><span class="vlist"><span style="top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-0.363em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.26808571428571426em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span style="top:-0.363em;margin-right:0.07142857142857144em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>.</p>
<p>For clarity, we use subscript <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span></span>, to count across parameter space in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.02691em;">w</span></span></span></span>, and this is not to be confused with superscript <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.65952em;"></span><span class="strut bottom" style="height:0.65952em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span>, used to count across each sampled member of the population of size <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.10903em;">N</span></span></span></span>. Combining the last two equations, we can update <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>μ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mu_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">μ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>σ</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.05724em;">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> at each generation via a gradient update.</p>
<p>We note that there is a connection between population-based REINFORCE, a population-based policy gradient method, and particular formulations of Evolution Strategies <dt-cite key="rechenberg1978evolutionsstrategien,schwefel1981numerical"></dt-cite>, namely ones that are not elitist. For instance, Natural Evolution Strategies (NES) <dt-cite key="pepg,wierstra2008natural"></dt-cite> and OpenAI-ES <dt-cite key="openai_es"></dt-cite> are closely based on Sec. 6 of REINFORCE. There is also a connection between natural gradients (computed using NES) and CMA-ES <dt-cite key="cmaes"></dt-cite>. We refer to Akimoto et al. <dt-cite key="akimoto2012theoretical"></dt-cite> for a detailed theoretical treatment and discussion about the connection between CMA-ES and natural gradients.</p>
<hr>
<h2>Experiments</h2>
<h3>Learning better legs for better gait</h3>
<p><em>RoboschoolAnt-v1</em></p>
<div style="text-align: center;">
<table style="width:100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: left;">Original <i>Ant</i> agent:</figcaption></td>
<td width="50%"><figcaption style="text-align: left;">Agent with learned design:</figcaption></td>
</tr></table>
<!--<video autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;"><source src="assets/mp4/ant_views.mp4" type="video/mp4"/></video>-->
<video class="b-lazy" data-src="assets/mp4/ant_views.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
</div>
<p>In this work, we experiment on continuous control environments from Roboschool<dt-cite key="roboschool"></dt-cite>, based on the open source Bullet<dt-cite key="pybullet"></dt-cite> physics engine, and the Box2D<dt-cite key="box2d"></dt-cite> section of the OpenAI Gym<dt-cite key="openai_gym"></dt-cite> set of environments. For simplicity, we first present results of anecdotal examples obtained over a single representative experimental run to convey qualitative results such as morphology and its relationship to performance. A more comprehensive quantitative study based on multiple runs using different random seeds will be presented in a later section.</p>
<p>The <em>RoboschoolAnt-v1</em><dt-fn>A compatible version of this environment is also available in PyBullet which was used for visualization.</dt-fn> environment features a four-legged agent called the <em>Ant</em>. The body is supported by 4 legs, and each leg consists of 3 parts which are controlled by 2 motor joints. The bottom right diagram in the below figure describes the initial orientation of the agent.<dt-fn>The length of each part of a leg is controlled by the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Δ</span><span class="mord mathit">x</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">\Delta y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">Δ</span><span class="mord mathit" style="margin-right:0.03588em;">y</span></span></span></span> distances from its joint connection. A size parameter also controls the radius of each leg part.</dt-fn></p>
<p>In our experiment, we keep the volumetric mass density of all materials, along with the parameters of the motor joints identical to the original environment, and allow the 36 parameters (3 parameters per leg part, 3 leg parts per leg, 4 legs in total) to be learned. In particular, we allow each part to be scaled to a range of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 75% of its original value. This allows us to keep the sign and direction for each part to preserve the original intended structure of the design.</p>
<div style="text-align: center;">
<img src="assets/img/ant_orig.jpg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Agent learning a policy to navigate forward in <i>RoboschoolAnt-v1</i> environment<dt-cite key="roboschool"></dt-cite>.</figcaption>
<img src="assets/img/ant_augment.jpg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Agent develops longer, thinner legs while supporting the same body during training.</figcaption>
</div>
<p>The above figure illustrates the learned agent design compared to the original design. With the exception of one leg part, it learns to develop longer, thinner legs while jointly learning to carry the body across the environment. While the original design is symmetric, the learned design breaks symmetry, and biases towards larger rear legs while jointly learning the navigation policy using an asymmetric body. The original agent achieved an average cumulative score of 3447 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 251 over 100 trials, compared to 5789 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 479 for an agent that learned a better body design.<dt-fn>In our experiment, we keep the volumetric mass density of all materials, along with the parameters of the motor joints identical to the original environment, and allow the 36 parameters (3 parameters per leg part, 3 leg parts per leg, 4 legs in total) to be learned. In particular, we allow each part to be scaled to a range of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 75% of its original value. This allows us to keep the sign and direction for each part to preserve the original intended structure of the design.</dt-fn></p>
<p><em>BipedalWalker-v2</em></p>
<p>The Bipedal Walker series of environments is based on the Box2D<dt-cite key="box2d"></dt-cite> physics engine. Guided by LIDAR sensors, the agent is required to navigate across an environment of randomly generated terrain within a time limit, without falling over. The agent's payload -- its head, is supported by 2 legs. The top and bottom parts of each leg is controlled by two motor joints. In the easier <em>BipedalWalker-v2</em><dt-cite key="bipedalwalker"></dt-cite> environment, the agent needs to travel across small random variations of a flat terrain. The task is considered solved if an agent obtains an average score greater than 300 points over 100 rollouts.</p>
<div style="text-align: center;">
<!--<video autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;"><source src="assets/mp4/biped_compare_vs_augment.mp4" type="video/mp4"/></video>-->
<video class="b-lazy" data-src="assets/mp4/biped_compare_vs_augment.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Learning a policy to navigate forward in <i>BipedalWalker-v2</i> environment (left). Agent learns a body to allow it to bounce forward efficiently (right).</figcaption>
</div>
<p>Keeping the head payload constant, and also keeping the density of materials and the configuration of the motor joints the same as the original environment, we only allow the lengths and widths for each of the 4 leg parts to be learnable, subject to the same range limit of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 75% of the original design. In the original environment, the agent learns a policy that is reminiscent of a joyful skip across the terrain, achieving an average score of 347. In the learned version, the agent's policy is to hop across the terrain using its legs as a pair of springs, achieving a higher average score of 359.</p>
<p>In our experiments, all agents were implemented using 3 layer fully-connected networks with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>tanh</mi></mrow><annotation encoding="application/x-tex">\tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mop">tanh</span></span></span></span> activations. The agent in <em>RoboschoolAnt-v1</em> has 28 inputs and 8 outputs, all bounded between <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span>, with hidden layers of 64 and 32 units. The agents in <em>BipedalWalker-v2</em> and <em>BipedalWalkerHardcore-v2</em> has 24 inputs and 4 outputs all bounded between <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">−</span><span class="mord mathrm">1</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">+</span><span class="mord mathrm">1</span></span></span></span>, with 2 hidden layers of 40 units each.</p>
<p>Our population-based training experiments were conducted on 96-CPU core machines. Following the approach described in <dt-cite key="stablees"></dt-cite>, we used a population size of 192, and had each agent perform the task 16 times with different initial random seeds. The agent's reward signal used by the policy gradient method is the average reward of the 16 rollouts. The most challenging BipedalWalkerHardcore agents were trained for 10000 generations, while the easier BipedalWalker and Ant agents were trained for 5000 and 3000 generations respectively. As done in <dt-cite key="stablees"></dt-cite>, we save the parameters of the agent that achieves the best average cumulative reward during its entire training history.</p>
<h3>Joint learning of body design facilitates policy learning</h3>
<p><em>BipedalWalkerHardcore-v2</em></p>
<div style="text-align: center;">
<!--<video autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;"><source src="assets/mp4/augmentbipedhard_all.mp4" type="video/mp4"/></video>-->
<video class="b-lazy" data-src="assets/mp4/augmentbipedhard_all.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Learned body to efficiently navigate <i>BipedalWalkerHardcore-v2</i><dt-cite key="bipedalwalkerhardcore"></dt-cite>.</figcaption>
</div>
<p>Learning a better version of an agent's body not only helps achieve better performance, but also enables the agent to jointly learn policies more efficiently. We demonstrate this in the much more challenging <em>BipedalWalkerHardcore-v2</em><dt-cite key="bipedalwalkerhardcore"></dt-cite> version of the task. Unlike the easier version, the agent must also learn to walk over obstacles, travel up and down hilly terrain, and even jump over pits. As of writing, two methods are reported to solve this task. Population-based training<dt-cite key="stablees"></dt-cite> (our baseline), solves this task in 40 hours on a 96-CPU machine, using a small feed forward policy network. A3C<dt-cite key="mnih2016asynchronous"></dt-cite> adapted for continuous control<dt-cite key="griffis2018"></dt-cite> solves the task in 48 hours on a 72-CPU machine, but requires an LSTM<dt-cite key="lstm"></dt-cite> policy network.</p>
<div style="text-align: center;">
<img src="assets/fig/population.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Population-based training curves for both versions of <i>BipedalWalkerHardcore-v2</i>.</figcaption>
<img src="assets/fig/avg_best.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Plot of performance of best agent in the population over 100 random trials. Original version solved under 4600 generations (40 hours); learnable one solved under 1400 generations (12 hours).</figcaption>
</div>
<p>In this environment, our agent generally learns to develop longer, thinner legs, with the exception in the rear leg where it developed a thicker lower limb to serve as useful stability function for navigation. Its front legs, which are smaller and more manoeuvrable, also act as a sensor for dangerous obstacles ahead that complement its LIDAR sensors. While learning to develop this newer structure, it jointly learns a policy to solve the task in 30% of the time it took the original, static version of the environment. The average scores over 100 rollouts for the learnable version is 335 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 37 compared to the baseline score of 313 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 53.</p>
<h3>Optimize for both the task and desired design properties</h3>
<div style="text-align: center;">
<img src="assets/fig/biped_results.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Summary of results for Bipedal Walker environments. Scaled Box2D dimensions reported.</figcaption>
</div>
<p>Allowing an agent to learn a better version of its body obviously enables it to achieve better performance. But what if we want to give back some of the additional performance gains, and also optimize also for desirable design properties that might not generally be beneficial for performance? For instance, we may want our agent to learn a design that utilizes the least amount of materials while still achieving satisfactory performance on the task. Here, we reward an agent for developing legs that are smaller in area, and augment its reward signal during training by scaling the rewards by a utility factor of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>+</mo><mi>log</mi><mo>(</mo><mfrac><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">g</mi><mtext> </mtext><mi mathvariant="normal">l</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi><mtext> </mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi></mtext></mrow><mrow><mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">w</mi><mtext> </mtext><mi mathvariant="normal">l</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">g</mi><mtext> </mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi></mtext></mrow></mfrac><mo>)</mo></mrow><annotation encoding="application/x-tex">1+\log(\frac{\text{orig leg area}}{\text{new leg area}})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.9322159999999999em;"></span><span class="strut bottom" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">+</span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.345em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="text mord scriptstyle cramped"><span class="mord mathrm">n</span><span class="mord mathrm">e</span><span class="mord mathrm" style="margin-right:0.01389em;">w</span><span class="mord mspace"> </span><span class="mord mathrm">l</span><span class="mord mathrm">e</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mspace"> </span><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.44610799999999995em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="text mord scriptstyle uncramped"><span class="mord mathrm">o</span><span class="mord mathrm">r</span><span class="mord mathrm">i</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mspace"> </span><span class="mord mathrm">l</span><span class="mord mathrm">e</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mord mspace"> </span><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">e</span><span class="mord mathrm">a</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>. We see that augmenting the reward encourages development of smaller legs:</p>
<div style="text-align: center;">
<!--<video autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;"><source src="assets/mp4/augmentbipedsmalllegs.mp4" type="video/mp4"/></video>-->
<video class="b-lazy" data-src="assets/mp4/augmentbipedsmalllegs.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Agent rewarded for smaller legs for the task in <i>BipedalWalker-v2</i> environment.</figcaption>
</div>
<p>This reward augmentation resulted in much a smaller agent that is still able to support the same payload. In the easier <em>BipedalWalker</em> task, given the simplicity of the task, the agent's leg dimensions eventually shrink to near the lower bound of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 25% of the original dimensions, with the exception of the heights of the top leg parts which settled at <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.36687em;"></span><span class="strut bottom" style="height:0.36687em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mrel">∼</span></span></span></span> 35% of the initial design, while still achieving an average (unaugmented) score of 323 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 68. For this task, the leg area used is 8% of the original design.</p>
<div style="text-align: center;">
<!--<video autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;"><source src="assets/mp4/augmentbipedhardsmalllegs_all.mp4" type="video/mp4"/></video>-->
<video class="b-lazy" data-src="assets/mp4/augmentbipedhardsmalllegs_all.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">When rewarded for small leg size, the agent learned a lean minimal design where every inch matters. It also learned movements that appear more insect-like. Here, the agent learns the smallest pair of legs that still can solve <i>BipedalWalkerHardcore-v2</i>.</figcaption>
</div>
<p>However, the agent is unable to solve the more difficult <em>BipedalWalkerHardcore</em> task using a similar small body structure, due to the various obstacles presented. Instead, it learns to set the widths of each leg part close to the lower bound, and instead learn the shortest heights of each leg part required to navigate, achieving a score of 312 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 69. Here, the leg area used is 27% of the original.</p>
<hr>
<h2>Results over Multiple Experimental Runs</h2>
<p>In the previous sections, for simplicity, we have presented results over a single representative experimental run to convey qualitative results such as morphology description corresponding to average score achieved. Running the experiment from scratch with a different random seed may generate different morphology designs and different policies that lead to different performance scores. To demonstrate that morphology learning does indeed improve the performance of the agent over multiple experimental runs, we run each experiment 10 times and report the full range of average scores obtained in the two tables below:</p>
<div style="text-align: center;">
<img src="assets/fig/multi_run_avg.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Summary of results for each experiment over 10 independent runs.</figcaption>
<img src="assets/fig/multi_run_full.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Full results from each of the 10 experimental trials. Each number is the average score of the trained agent over 100 rollouts in the environment.</figcaption>
</div>
<p>From multiple independent experimental runs, we see that morphology learning consistently produces higher scores over the normal task.</p>
<p>We also visualize the variations of morphology designs over different runs in the following figure to get a sense of the variations of morphology that can be discovered during training:</p>
<div style="text-align: center;">
<img src="assets/fig/multi_run_results.png" style="display: block; margin: auto; width: 100%;"/>
<figcaption style="text-align: left;">Examples of learned morphology in Run #9. On Biped (top), the agent develops a thicker but short rear lower limb. On Biped Hardcore (bottom), it also develops a larger rear leg, but its upper thigh is noticeably larger.</figcaption>
</div>
<p>As these models may take up to several days to train for a particular experiment on a powerful 96-core CPU machine, it may be costly for the reader to fully reproduce the variation of results here, especially when 10 machines running the same experiment with different random seeds are required. We also include all pretrained models from multiple independent runs in the GitHub repository containing the code to reproduce this paper. The interested reader can examine the variations in more detail using the pretrained models.</p>
<div style="text-align: center;">
<table style="width:100%;" cellspacing="0" cellpadding="0"><tr>
  <tr>
  <td><video class="b-lazy" data-src="assets/mp4/augmentbipedhard.a.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video></td>
  <td><video class="b-lazy" data-src="assets/mp4/augmentbipedhard.b.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video></td>
  <td><video class="b-lazy" data-src="assets/mp4/augmentbipedhard.c.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video></td>
  </tr>
  <tr>
  <td><video class="b-lazy" data-src="assets/mp4/augmentbipedhardsmalllegs.a.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video></td>
  <td><video class="b-lazy" data-src="assets/mp4/augmentbipedhardsmalllegs.b.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video></td>
  <td><video class="b-lazy" data-src="assets/mp4/augmentbipedhardsmalllegs.c.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video></td>
  </tr>
</table>
<figcaption style="text-align: left;">Variations of designs in <i>BipedalWalkerHardcore-v2</i> (top) and also rewarding the agent for smaller legs (bottom).</figcaption>
</div>
<hr>
<h2>Discussion and Future Work</h2>
<p>We have shown that allowing a simple population-based policy gradient method to learn not only the policy, but also a small set of parameters describing the environment, such as its body, offer many benefits. By allowing the agent's body to adapt to its task within some constraints, it can learn policies that are not only better for its task, but also learn them more quickly.</p>
<p>The agent may discover design principles during this process of joint body and policy learning. In both <em>RoboschoolAnt</em> and <em>BipedalWalker</em> experiments, the agent has learned to break symmetry and learn relatively larger rear limbs to facilitate their navigation policies. While also optimizing for material usage for <em>BipedalWalker</em>'s limbs, the agent learns that it can still achieve the desired task even by setting the size of its legs to the minimum allowable size. Meanwhile, for the much more difficult <em>BipedalWalkerHardcore-v2</em> task, the agent learns the appropriate length of its limbs required for the task while still minimizing the material usage.</p>
<p>This approach may lead to useful applications in machine learning-assisted design, in the spirit of<dt-cite key="carter2017"></dt-cite>. Game designers can optimize the designs of game character assets while at the same time being able to constrain the characters to keep the essence of their original forms. Optimizing character design may complement existing work on machine learning-assisted procedural content generation for game design<dt-cite key="togelius2008experiment,summerville2018procedural,volz2018evolving,guzdial2018co"></dt-cite>. By framing the approach within the popular OpenAI Gym framework, design firms can create more realistic environments -- for instance, incorporate strength of materials, safety factors, malfunctioning of components under stressed conditions, and plug existing algorithms into this framework to optimize also for design aspects such as energy usage, easy-of-manufacturing, or durability. The designer may even incorporate aesthetic constraints such as symmetry and aspect ratios that suits her design sense.</p>
<p>In this work we have only explored using a simple population-based policy gradient method<dt-cite key="williams1992"></dt-cite> for learning. State-of-the-art model-free RL algorithms, such as TRPO<dt-cite key="schulman2015trust"></dt-cite> and PPO <dt-cite key="schulman2017proximal"></dt-cite> work well when our agent is presented with a well-designed dense reward signal, while population-based methods offer computational advantages for sparse-reward problems<dt-cite key="openai_es,DeepNeuroevolution2017"></dt-cite>. In our setting, since the body design is parameterized by a small set of learnable parameters and is only set once at the beginning of a rollout, the problem of learning the body along with the policy becomes more sparse. In principle, we could allow an agent to augment its body <em>during</em> a rollout to obtain a dense reward signal, but we find this unpractical for realistic problems. Future work may look at separating the learning from dense-rewards and sparse-rewards into an inner loop and outer loop, and also examine differences in performance and behaviours in structures learned using various different learning approaches.</p>
<p>Separation of policy learning and body design into inner loop and outer loop will also enable the incorporation of evolution-based approaches to tackle the vast search space of morphology design, while utilizing efficient RL-based methods for policy learning. The limitations of the current approach is that our RL algorithm can learn to optimize only existing design properties of an agent's body, rather than learn truly novel morphology in the spirit of Karl Sims' <em>Evolving Virtual Creatures</em><dt-cite key="sims1994evolving"></dt-cite>.</p>
<p>Nevertheless, our approach of optimizing the specifications of an existing design might be more practical for many applications. An evolutionary algorithm might come up with trivial designs and corresponding simple policies that outperform designs we actually want -- for instance, a large ball that rolls forward will easily outperforming the best bipedal walkers, but this might not be useful to a game designer who simply wants to optimize the dimensions of an existing robot character for a video game. Due to the vast search space of morphology, a search algorithm can easily come up with a trivial, but unrealistic or unusable design that exploits its simulation environment<dt-cite key="lehman2018surprising"></dt-cite>, which may be why subsequent morphology-evolution approaches constrain the search space of the agent's morphology, such as constraining to the space of soft-body voxels<dt-cite key="cheney2013unshackling"></dt-cite> or constraining to a set of possible pipe frame connection settings<dt-cite key="jansen2008strandbeests"></dt-cite>. We note that unrealistic designs may also result in our approach, if we do not constrain the learned dimensions to be within <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 75% of its original value. For some interesting examples of what REINFORCE discovers without any constraints, please see the next section.</p>
<p>Just as REINFORCE<dt-cite key="williams1992"></dt-cite> can also be applied to the discrete search problem of neural network architecture designs<dt-cite key="zoph2016neural"></dt-cite>, similar RL-based approaches could be used for novel morphology design -- not simply for improving an existing design like in this work. We believe the ability to learn useful morphology is an important area for the advancement of AI. Although morphology learning originally initiated from the field of evolutionary computation, we hope this work will engage the RL community to investigate the concept further and encourage idea exchange across communities.</p>
<hr>
<h2>Bloopers</h2>
<p>For those of you who made it this far, we would like to share some “negative results” of things that we tried but didn't work. In the experiments, we constrain the elements in the modified design to be <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>±</mo></mrow><annotation encoding="application/x-tex">\pm</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord">±</span></span></span></span> 75% of the original design's values. We accomplish this by defining a scaling factor for each learnable parameter as <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mi mathvariant="normal">.</mi><mn>0</mn><mo>+</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>7</mn><mn>5</mn><mi>tanh</mi><mo>(</mo><msub><mi>w</mi><mi>k</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">1.0+0.75 \tanh(w_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mbin">+</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">w_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> is the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>k</mi><mrow><mtext><mi mathvariant="normal">t</mi><mi mathvariant="normal">h</mi></mtext></mrow></msup></mrow><annotation encoding="application/x-tex">k^{\text{th}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03148em;">k</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="text mord scriptstyle uncramped"><span class="mord mathrm">t</span><span class="mord mathrm">h</span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> element of the environment parameter vector, and multiply this scaling factor to the original design's value, and find that this approach works well as it usually preserves the intention and <em>essence</em> of the original design.</p>
<p>We also tried to let the RL algorithm discover new designs without any constraints, and found that it would usually create longer rear legs during the initial learning phase designed so it can tumble over further down the map to achieve higher rewards.</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/augmentbipedsmalllegs.lognormal.blooper.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">Without any design constraints, it develops very long rear legs so it can finish further down the map.</figcaption>
</div>
<p>Using a lognormal scaling factor of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>exp</mi><mo>(</mo><msub><mi>w</mi><mi>k</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\exp(w_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.02691em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span> made it easier for the RL algorithm to come up with an extremely tall bipedal walker agent that “solves” the task by simply falling over and landing at the exit:</p>
<div style="text-align: center;">
<video class="b-lazy" data-src="assets/mp4/augmentbipedhard.lognormal.blooper.mp4" type="video/mp4" autoplay muted playsinline loop style="display: block; margin: auto; width: 100%;" ></video>
<figcaption style="text-align: left;">If we remove all design constraints, the optimizer came up with a really tall bipedal walker robot that “solves” the task by simply falling over and landing near the exit.</figcaption>
</div>
</dt-article>
<dt-appendix>
<h2>Acknowledgments</h2>
<p>We would like to thank the three reviewers from Artificial Life Journal, Luke Metz, Douglas Eck, Janelle Shane, Julian Togelius, Jeff Clune, and Kenneth Stanley for their thoughtful feedback and conversation. All experiments were performed on CPU machines provided by Google Cloud.</p>
<p>This article was prepared using the <a href="https://distill.pub">Distill</a> <a href="https://github.com/distillpub/template">template</a>.</p>
<h3 id="citation">Citation</h3>
<p>For attribution in academic contexts, please cite this work as</p>
<pre class="citation short">David Ha, "Reinforcement Learning for Improving Agent Design", 2018.</pre>
<p>BibTeX citation</p>
<pre class="citation long">@article{Ha2018designrl,
  author = {David Ha},
  title  = {Reinforcement Learning for Improving Agent Design},
  eprint = {arXiv:1810.03779},
  url    = {https://designrl.github.io},
  note   = "\url{https://designrl.github.io}",
  year   = {2018}
}</pre>
<h2>Open Source Code</h2>
<p>The instructions to reproduce the experiments in this work is available <a href="https://github.com/hardmaru/astool">here</a>.</p>
<h2>Reuse</h2>
<p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a href="https://github.com/designrl/designrl.github.io">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by the citations in their caption.</p>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
@ARTICLE{openai_gym,
  author = {Brockman, G. and Cheung, V. and Pettersson, L. and Schneider, J. and Schulman, J. and Tang, J. and Zaremba, W.},
  title={OpenAI Gym},
  journal={Preprint arXiv:1606.01540},
  year = 2016,
  month = jun,
  url = {https://arxiv.org/abs/1606.01540},
}
@ARTICLE{roboschool,
  author = {Oleg Klimov and John Schulman},
  title={Roboschool},
  journal={OpenAI Blog},
  month=may,
  year=2017,
  url = {https://blog.openai.com/roboschool/},
}
  @article{bipedalwalker,
    author = {Oleg Klimov},
    title = {BipedalWalker-v2},
    year = 2016,
    url = {https://gym.openai.com/envs/BipedalWalker-v2/},
    journal = {OpenAI Gym},
    urldate = {2017-11-01}
  }
  @article{bipedalwalkerhardcore,
    author = {Oleg Klimov},
    title = {BipedalWalkerHardcore-v2},
    year = 2016,
    url = {https://gym.openai.com/envs/BipedalWalkerHardcore-v2/},
    journal = {OpenAI Gym},
    urldate = {2017-11-01}
  }
@ARTICLE{pybullet,
  author = {Erwin Coumans},
  title={PyBullet Physics Environment},
  journal={GitHub Repo},
  year=2017,
  url = {https://github.com/bulletphysics/bullet3},
}
@ARTICLE{griffis2018,
  author = {David Griffis},
  title={RL A3C Pytorch Continuous},
  journal="GitHub Repo",
  year=2018,
  url = "https://github.com/dgriff777/a3c_continuous",
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}
@article{lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@ARTICLE{box2d,
  title={Box2d: A 2d physics engine for games},
  author={Catto, Erin},
  journal={GitHub Repo},
  url={https://github.com/pybox2d/pybox2d},
  year={2011},
}
@article{ha2018computational,
  title={Computational co-optimization of design parameters and motion trajectories for robotic systems},
  author={Ha, Sehoon and Coros, Stelian and Alspach, Alexander and Kim, Joohyung and Yamane, Katsu},
  journal={The International Journal of Robotics Research},
  pages={0278364918771172},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{corucci2018evolving,
  title={Evolving soft locomotion in aquatic and terrestrial environments: effects of material properties and environmental transitions},
  author={Corucci, Francesco and Cheney, Nick and Giorgio-Serchi, Francesco and Bongard, Josh and Laschi, Cecilia},
  journal={Soft robotics},
  volume={5},
  number={4},
  pages={475--495},
  year={2018},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}
@article{hyperneat,
 author = {Gauci, Jason and Stanley, Kenneth O.},
 title = {Autonomous Evolution of Topographic Regularities in Artificial Neural Networks},
 journal = {Neural Computation},
 issue_date = {July 2010},
 volume = {22},
 number = {7},
 month = jul,
 year = {2010},
 issn = {0899-7667},
 pages = {1860--1898},
 numpages = {39},
 url = {http://eplex.cs.ucf.edu/papers/gauci_nc10.pdf},
 doi = {10.1162/neco.2010.06-09-1042},
 acmid = {1810223},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@Article{neat,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  publisher={MIT Press}
}
@article{jansen2008strandbeests,
  title={Strandbeests},
  author={Jansen, Theo},
  journal={Architectural Design},
  volume={78},
  number={4},
  pages={22--27},
  year={2008},
  publisher={Wiley Online Library}
}
@inproceedings{sims1994evolving,
  title={Evolving virtual creatures},
  author={Sims, Karl},
  booktitle={Proceedings of the 21st annual conference on Computer graphics and interactive techniques},
  pages={15--22},
  year={1994},
  organization={ACM}
}
@article{sims1994evolving_MIT,
  title={Evolving 3D morphology and behavior by competition},
  author={Sims, Karl},
  journal={Artificial life},
  volume={1},
  number={4},
  pages={353--372},
  year={1994},
  publisher={MIT Press}
}
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}
@inproceedings{mcgeer1990passive,
  title={Passive walking with knees},
  author={McGeer, Tad},
  booktitle={Proceedings., IEEE International Conference on Robotics and Automation},
  pages={1640--1645},
  year={1990},
  organization={IEEE}
}
@article{williams1992,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  url={http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf},
  publisher={Springer}
}
@article{pepg,
  title={Parameter-exploring policy gradients},
  author={Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={23},
  number={4},
  pages={551--559},
  year={2010},
  publisher={Elsevier}
}
@inproceedings{wierstra2008natural,
  title={Natural evolution strategies},
  author={Wierstra, Daan and Schaul, Tom and Peters, Jan and Schmidhuber, Juergen},
  booktitle={Evolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelligence). IEEE Congress on},
  pages={3381--3387},
  year={2008},
  organization={IEEE}
}
  @article{openai_es,
    title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
   author = {Salimans, T. and Ho, J. and Chen, X. and Sidor, S. and Sutskever, I.},
  journal={Preprint arXiv:1703.03864},
    year={2017},
    url={https://arxiv.org/abs/1703.03864}
  }
  @article{visuales,
    title={A Visual Guide to Evolution Strategies},
    author = {Ha, D.},
    journal={blog.otoro.net},
    year={2017},
    url="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/"
  }
  @article{stablees,
    title={Evolving Stable Strategies},
    author = {Ha, D.},
    journal={blog.otoro.net},
    year={2017},
    url="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/"
  }
  @article{cmaes,
   author = {Hansen, Nikolaus and Ostermeier, Andreas},
   title = {Completely Derandomized Self-Adaptation in Evolution Strategies},
   journal = {Evolutionary Computation},
   issue_date = {June 2001},
   volume = {9},
   number = {2},
   month = jun,
   year = {2001},
   issn = {1063-6560},
   pages = {159--195},
   numpages = {37},
   url = {http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf},
   doi = {10.1162/106365601750190398},
   acmid = {1108843},
   publisher = {MIT Press},
   address = {Cambridge, MA, USA},
  }
@article{akimoto2012theoretical,
  title={Theoretical foundation for CMA-ES from information geometry perspective},
  author={Akimoto, Youhei and Nagata, Yuichi and Ono, Isao and Kobayashi, Shigenobu},
  journal={Algorithmica},
  volume={64},
  number={4},
  pages={698--716},
  year={2012},
  publisher={Springer}
}
@article{wilson2002six,
  title={Six views of embodied cognition},
  author={Wilson, Margaret},
  journal={Psychonomic bulletin & review},
  volume={9},
  number={4},
  pages={625--636},
  year={2002},
  publisher={Springer}
}
@article{anderson2003embodied,
  title={Embodied cognition: A field guide},
  author={Anderson, Michael L},
  journal={Artificial intelligence},
  volume={149},
  number={1},
  pages={91--130},
  year={2003}
}
@article{mahon2008critical,
  title={A critical look at the embodied cognition hypothesis and a new proposal for grounding conceptual content},
  author={Mahon, Bradford Z and Caramazza, Alfonso},
  journal={Journal of physiology-Paris},
  volume={102},
  number={1-3},
  pages={59--70},
  year={2008},
  publisher={Elsevier}
}
@book{shapiro2010embodied,
  title={Embodied cognition},
  author={Shapiro, Lawrence},
  year={2010},
  publisher={Routledge}
}
@article{gover1996embodied,
  title={The Embodied Mind: Cognitive Science and Human Experience (Book)},
  author={Gover, Mark R},
  journal={Mind, Culture, and Activity},
  volume={3},
  number={4},
  pages={295--299},
  year={1996},
  publisher={Taylor & Francis}
}
@book{pfeifer2006body,
  title={How the body shapes the way we think: a new view of intelligence},
  author={Pfeifer, Rolf and Bongard, Josh},
  year={2006},
  publisher={MIT press}
}
@article{bresadola1998medicine,
  title={Medicine and science in the life of Luigi Galvani (1737--1798)},
  author={Bresadola, Marco},
  journal={Brain Research Bulletin},
  volume={46},
  number={5},
  pages={367--380},
  year={1998},
  publisher={Elsevier}
}
@article{beal2006passive,
  title={Passive propulsion in vortex wakes},
  author={Beal, DN and Hover, FS and Triantafyllou, MS and Liao, JC and Lauder, GV},
  journal={Journal of Fluid Mechanics},
  volume={549},
  pages={385--402},
  year={2006},
  publisher={Cambridge University Press}
}
@article{tricoli2005short,
  title={Short-term effects on lower-body functional power development: weightlifting vs. vertical jump training programs},
  author={Tricoli, Valmor and Lamas, Leonardo and Carnevale, Roberto and Ugrinowitsch, Carlos},
  journal={The Journal of Strength & Conditioning Research},
  volume={19},
  number={2},
  pages={433--437},
  year={2005},
  publisher={LWW}
}
@article{raglin1990exercise,
  title={Exercise and mental health},
  author={Raglin, John S},
  journal={Sports Medicine},
  volume={9},
  number={6},
  pages={323--329},
  year={1990},
  publisher={Springer}
}
@article{deslandes2009exercise,
  title={Exercise and mental health: many reasons to move},
  author={Deslandes, Andr{\'e}a and Moraes, Helena and Ferreira, Camila and Veiga, Heloisa and Silveira, Heitor and Mouta, Raphael and Pompeu, Fernando AMS and Coutinho, Evandro Silva Freire and Laks, Jerson},
  journal={Neuropsychobiology},
  volume={59},
  number={4},
  pages={191--198},
  year={2009},
  publisher={Karger Publishers}
}
@article{bongard2011morphological,
  title={Morphological change in machines accelerates the evolution of robust behavior},
  author={Bongard, Josh},
  journal={Proceedings of the National Academy of Sciences},
  volume={108},
  number={4},
  pages={1234--1239},
  year={2011},
  publisher={National Acad Sciences}
}
@book{leger1999automated,
  title={Automated synthesis and optimization of robot configurations: an evolutionary approach},
  author={Leger, Chris},
  year={1999},
  publisher={Carnegie Mellon University USA}
}
@inproceedings{cheney2013unshackling,
  title={Unshackling evolution: evolving soft robots with multiple materials and a powerful generative encoding},
  author={Cheney, Nick and MacCurdy, Robert and Clune, Jeff and Lipson, Hod},
  booktitle={Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages={167--174},
  year={2013},
  organization={ACM}
}
@inproceedings{prokopenko2006evolving,
  title={Evolving spatiotemporal coordination in a modular robotic system},
  author={Prokopenko, Mikhail and Gerasimov, Vadim and Tanev, Ivan},
  booktitle={International Conference on Simulation of Adaptive Behavior},
  pages={558--569},
  year={2006},
  organization={Springer}
}
@article{zykov2007evolved,
  title={Evolved and designed self-reproducing modular robotics},
  author={Zykov, Victor and Mytilinaios, Efstathios and Desnoyer, Mark and Lipson, Hod},
  journal={IEEE Transactions on robotics},
  volume={23},
  number={2},
  pages={308--319},
  year={2007},
  publisher={IEEE}
}
@inproceedings{ostergaard2003evolving,
  title={Evolving control for modular robotic units},
  author={Ostergaard, Esben Hallundbok and Lund, Henrik Hautop},
  booktitle={Computational Intelligence in Robotics and Automation, 2003. Proceedings. 2003 IEEE International Symposium on},
  volume={2},
  pages={886--892},
  year={2003},
  organization={IEEE}
}
@article{collins2001three,
  title={A three-dimensional passive-dynamic walking robot with two legs and knees},
  author={Collins, Steven H and Wisse, Martijn and Ruina, Andy},
  journal={The International Journal of Robotics Research},
  volume={20},
  number={7},
  pages={607--615},
  year={2001},
  publisher={SAGE Publications}
}
@article{collins2005efficient,
  title={Efficient bipedal robots based on passive-dynamic walkers},
  author={Collins, Steve and Ruina, Andy and Tedrake, Russ and Wisse, Martijn},
  journal={Science},
  volume={307},
  number={5712},
  pages={1082--1085},
  year={2005},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{paul2004morphology,
  title={Morphology and computation},
  author={Paul, Chandana},
  booktitle={Proceedings of the International Conference on the Simulation of Adaptive Behaviour Los Angeles, CA, USA},
  pages={33--38},
  year={2004}
}
@article{carter2016,
  author = {Carter, Shan and Ha, David and Johnson, Ian and Olah, Chris},
  title = {Experiments in Handwriting with a Neural Network},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/handwriting},
  doi = {10.23915/distill.00004}
}
@article{carter2017,
  author = {Carter, Shan and Nielsen, Michael},
  title = {Using Artificial Intelligence to Augment Human Intelligence},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/aia},
  doi = {10.23915/distill.00009}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  url = {https://arxiv.org/abs/1707.06347},
  year={2017}
}
@ARTICLE{DeepNeuroevolution2017,
    title = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  author={Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={Preprint arXiv:1712.06567},
  url = {https://arxiv.org/abs/1712.06567},
archivePrefix = "arXiv",
   eprint = {1712.06567},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
     year = 2017,
    month = dec,
}
@article{lehman2018surprising,
  title={The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities},
  author={Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Beaulieu, Julie and Bentley, Peter J and Bernard, Samuel and Belson, Guillaume and Bryson, David M and Cheney, Nick and others},
  journal={arXiv preprint arXiv:1803.03453},
  url = {https://arxiv.org/abs/1803.03453},
  year={2018}
}
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  url = {https://arxiv.org/abs/1611.01578},
  year={2016}
}
@inproceedings{togelius2008experiment,
  title={An experiment in automatic game design},
  author={Togelius, Julian and Schmidhuber, Jurgen},
  booktitle={Computational Intelligence and Games, 2008. CIG'08. IEEE Symposium On},
  pages={111--118},
  year={2008},
  organization={IEEE}
}
@book{millington2009artificial,
  title={Artificial intelligence for games},
  author={Millington, Ian and Funge, John},
  year={2009},
  publisher={CRC Press}
}
@article{summerville2018procedural,
  title={Procedural content generation via machine learning (PCGML)},
  author={Summerville, Adam and Snodgrass, Sam and Guzdial, Matthew and Holmgard, Christoffer and Hoover, Amy K and Isaksen, Aaron and Nealen, Andy and Togelius, Julian},
  journal={IEEE Transactions on Games},
  year={2018},
  publisher={IEEE}
}
@article{volz2018evolving,
  title={Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network},
  author={Volz, Vanessa and Schrum, Jacob and Liu, Jialin and Lucas, Simon M and Smith, Adam and Risi, Sebastian},
  journal={arXiv preprint arXiv:1805.00728},
  url = {https://arxiv.org/abs/1805.00728},
  year={2018}
}
@article{guzdial2018co,
  title={Co-Creative Level Design via Machine Learning},
  author={Guzdial, Matthew and Liao, Nicholas and Riedl, Mark},
  journal={arXiv preprint arXiv:1809.09420},
  url = {https://arxiv.org/abs/1809.09420},
  year={2018}
}
@inproceedings{ha2017joint,
  title={Joint optimization of robot design and motion parameters using the implicit function theorem},
  author={Ha, Sehoon and Coros, Stelian and Alspach, Alexander and Kim, Joohyung and Yamane, Katsu},
  booktitle={Robotics: Science and Systems},
  year={2017}
}
@article{schaff2018jointly,
  title={Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning},
  author={Schaff, Charles and Yunis, David and Chakrabarti, Ayan and Walter, Matthew R},
  journal={arXiv preprint arXiv:1801.01432},
  url = {https://arxiv.org/abs/1801.01432},
  year={2018}
}
@article{
schaff2018jointly_iclr_workshop,
title={Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning},
author={Charles Schaff and David Yunis and Ayan Chakrabarti and Matthew R. Walter},
journal={ICLR 2018 Workshop},
year={2018},
url="https://openreview.net/forum?id=SyfiiMZA-",
}
@article{lipson2000automatic,
  title={Automatic design and manufacture of robotic lifeforms},
  author={Lipson, Hod and Pollack, Jordan B},
  journal={Nature},
  volume={406},
  number={6799},
  pages={974},
  year={2000},
  publisher={Nature Publishing Group}
}
@inproceedings{szerlip2013indirectly,
  title={Indirectly Encoded Sodarace for Artificial Life.},
  author={Szerlip, Paul A and Stanley, Kenneth O},
  booktitle={ECAL},
  pages={218--225},
  year={2013},
  organization={MIT Press},
  url="http://eplex.cs.ucf.edu/ecal13/demo/PCA.html",
}
@inproceedings{szerlip2014steps,
  title={Steps toward a modular library for turning any evolutionary domain into an online interactive platform},
  author={Szerlip, Paul A and Stanley, Kenneth O},
  booktitle={H. Sayama and J. Rieffel and S. Risi and R. Doursat and H. Lipson, Artificial life 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems},
  pages={900--907},
  year={2014},
  organization={Citeseer}
}
@article{moore2014evolutionary,
  title={Evolutionary robotics on the Web with WebGL and Javascript},
  author={Moore, Jared and Clark, Anthony and McKinley, Philip},
  journal={arXiv preprint arXiv:1406.3337},
  url="http://jaredmmoore.com/WebGL_Visualizer/visualizer.html",
  year={2014}
}
  @misc{boxcar2d,
    author = {Ryan Weber },
    title = {BoxCar2D},
    year = 2010,
    url = {http://boxcar2d.com/about.html},
    urldate = {2018-11-01}
  }
@article{agrawal2014diverse,
  title={Diverse Motions and Character Shapes for Simulated Skills},
  author={Agrawal, Shailen and Shen, Shuo and van de Panne, Michiel},
  journal={IEEE transactions on visualization and computer graphics},
  volume={20},
  number={10},
  pages={1345--1355},
  year={2014},
  publisher={IEEE}
}
@article{geijtenbeek2013flexible,
  title={Flexible muscle-based locomotion for bipedal creatures},
  author={Geijtenbeek, Thomas and Van De Panne, Michiel and Van Der Stappen, A Frank},
  journal={ACM Transactions on Graphics (TOG)},
  volume={32},
  number={6},
  pages={206},
  year={2013},
  publisher={ACM}
}
@article{auerbach2014environmental,
  title={Environmental influence on the evolution of morphological complexity in machines},
  author={Auerbach, Joshua E and Bongard, Josh C},
  journal={PLoS computational biology},
  volume={10},
  number={1},
  pages={e1003399},
  year={2014},
  publisher={Public Library of Science}
}
@inproceedings{auerbach2012relationship,
  title={On the relationship between environmental and morphological complexity in evolved robots},
  author={Auerbach, Joshua E and Bongard, Joshua C},
  booktitle={Proceedings of the 14th annual conference on Genetic and evolutionary computation},
  pages={521--528},
  year={2012},
  organization={ACM}
}
@inproceedings{auerbach2010evolving,
  title={Evolving CPPNs to grow three-dimensional physical structures},
  author={Auerbach, Joshua E and Bongard, Josh C},
  booktitle={Proceedings of the 12th annual conference on Genetic and evolutionary computation},
  pages={627--634},
  year={2010},
  organization={ACM}
}
@inproceedings{auerbach2010dynamic,
  title={Dynamic resolution in the co-evolution of morphology and control},
  author={Auerbach, Joshua E and Bongard, Josh C},
  booktitle={Artificial Life XII: Proceedings of the Twelfth International Conference on the Synthesis and Simulation of Living Systems},
  pages={451--458},
  year={2010},
  organization={MIT Press}
}
@inproceedings{auerbach2014robogen,
  title={Robogen: Robot generation through artificial evolution},
  author={Auerbach, Joshua and Aydin, Deniz and Maesani, Andrea and Kornatowski, Przemyslaw and Cieslewski, Titus and Heitz, Gregoire and Fernando, Pradeep and Loshchilov, Ilya and Daler, Ludovic and Floreano, Dario},
  booktitle={Artificial Life Conference Proceedings 14},
  pages={136--137},
  year={2014},
  organization={MIT Press}
}
@inproceedings{auerbach2011evolving,
  title={Evolving complete robots with CPPN-NEAT: the utility of recurrent connections},
  author={Auerbach, Joshua E and Bongard, Josh C},
  booktitle={Proceedings of the 13th annual conference on Genetic and evolutionary computation},
  pages={1475--1482},
  year={2011},
  organization={ACM}
}
@incollection{rechenberg1978evolutionsstrategien,
  title={Evolutionsstrategien},
  author={Rechenberg, Ingo},
  editor={Schneider, B and Ranft, U},
  booktitle={Simulationsmethoden in der Medizin und Biologie},
  pages={83--114},
  year={1978},
  publisher={Springer}
}
@book{schwefel1981numerical,
  title={Numerical optimization of computer models},
  author={Schwefel, Hans-Paul},
  year={1981},
  publisher={John Wiley and Sons, Inc.}
}
</script>
<script src="lib/blazy.js"></script>
<script>
  // blazy code
  var bLazy = new Blazy({
    success: function(){
      updateCounter();
    }
  });

  // not needed, only here to illustrate amount of loaded images
  var imageLoaded = 0;

  function updateCounter() {
    imageLoaded++;
    console.log("blazy image loaded: "+imageLoaded);
  }
</script>